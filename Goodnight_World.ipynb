{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgGxuhX/msPp4RGPXUSMoH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will **create a neural network** and give it an environment. I'll try to write down what I understood from this work."
      ],
      "metadata": {
        "id": "3TTC7LHQi2Iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing TensorFlow and checking its version\n",
        "\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4vjXq9ujAdI",
        "outputId": "38b183bf-8f58-4cf2-8910-4c2dbeb9f03b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The MNIST Dataset is a collection of handwritten numbers used to train machines and neural networks\n",
        "\n",
        "The pixels in the image have an intensity value from **0** (Black) to **255** (White).\n",
        "\n",
        "We will scale the values to a range of **0** (Black) to **1** (White) to make it easier for the machines to learn. At the same time, previousy **integers** now become **float** numbers."
      ],
      "metadata": {
        "id": "MRzNxTMwj-pL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the MNIST Dataset and scaling to a range of 0 to 1\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "metadata": {
        "id": "hcVRobE7jYcX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a **Sequential** model.\n",
        "\n",
        "A Sequential model works as such :\n",
        "\n",
        "- Data flows through layers one at a time\n",
        "> The outside layer will have an operation on the data entered. It will come through the next layers until it reaches the core, where the data will be completely transformed and understandable by the machine.\n",
        "- You can choose each layer's operation and the order of the layers. Think of it as a sphere with layers and a core : [Example](https://i.ibb.co/MVgFmXb/opera-FTm-Mr-A4-TJp.png)\n",
        "\n",
        "# In our current code\n",
        "\n",
        "We have 4 layers that will work as such :\n",
        "\n",
        "- Layer 1: Flatten\n",
        "> The flatten layer takes our input image (in this case a 28x28 image of a single number) and transforms it into an array of length 784\n",
        "`28 * 28 = 784`. This way, our 2D image becomes a 1D character chain and can be read by our second layer. This function takes one parameter, the input_shape, defining the image size\n",
        "- Layer 2: Dense\n",
        "> Our dense layer has neurons, which will be fully connected to the output array from the flatten layer as such :\n",
        "Each neuron is connected to each value in the array. In this case we have `128 * 784 = 100352` connections, each connection having its own weight, a value representing the importance of the connection. Each neuron will have a collective value based on a sum of all the array values' . We also have a bias factor taking errors in count, but it isnt important for now.\n",
        "The mathematical operation for a single neuron goes as such :\n",
        "```\n",
        "Output = 128 * ( NonLinearityFunc ( âˆ‘ * n * ( Weight * Brightness ) + Bias ) )\n",
        "```\n",
        "> In normal terms, this means that we will multiply the brightness by its weight and add a small bias for every connection existing per neuron, times the amount of neurons. The Non-Linearity function is ReLU in this case, however, I cannot clarify how it works.\n",
        "This function takes two parameters : the activation type and number of neurons.\n",
        "\n",
        "- Layer 3: Dropout\n",
        "> For your sake and mine, this layer is a lot simpler !! This layer, will take randomly remove some neurons or set their value to 0, to prevent the network being too dependent on some neurons and to provide some randomness, making a more robust network. We can set a probability of neurons dropping out, in this case 0.2 (Probability parameter).\n",
        "\n",
        "- Layer 4: Dense (without activation)\n",
        "> This layer will transform our data back into a linear value. The value 10 means that the output will have 10 neurons to classify the \"answers\" in (in this case from 0 to 9)."
      ],
      "metadata": {
        "id": "CZP9ssxJlQLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the Sequential model.\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])"
      ],
      "metadata": {
        "id": "SKSHyd5IlU6W"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now **make predictions** with our neural network :\n",
        "\n",
        "Our untrained model will make predictions based on the first entry from the training set (in our case a handrwitten numbers list)\n",
        "\n",
        "For each image the neural network returns a vector with probabilities for each class. This is the soft output, the \"raw\" scores before classification. They represent the unnormalized predictions for each class."
      ],
      "metadata": {
        "id": "sIECwqL35Ien"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model(x_train[:1]).numpy()\n",
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pVk5ZZI5Qfv",
        "outputId": "2f16edfe-5abc-4136-d7d4-91b206cf4d31"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.35829848, -0.39773715,  0.7350598 , -0.11959001, -0.42246163,\n",
              "        -0.15682825,  0.11566684, -0.18161123,  0.6457797 , -0.5518897 ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now classify this vector and convert these \"logits\" into predictions."
      ],
      "metadata": {
        "id": "mTEH62Za6eZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.nn.softmax(predictions).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fW56M9Wc6o0f",
        "outputId": "d599e495-c5d1-4ea6-cf95-6c6af45ab3c5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.12977692, 0.0609334 , 0.18915719, 0.08047355, 0.05944532,\n",
              "        0.07753196, 0.10181784, 0.07563411, 0.17300116, 0.05222853]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will evaluate how well the predictions match the actual images in the dataset using the loss function. We will provide the labels ( the different classes, in this case our numbers, that the results should be put into) and the predictions after the softmax function. We will get a scalar loss (a calculation of how wrong a prediction is compared to the actual result) and we will use a negative log probability :\n",
        "It will measure the confidence of the model in its prediction. If the model is confident and correct, the loss is close to zero. If it's uncertain or wrong, the loss is higher.\n",
        "If the model is untrained, it will have an initial loss since it makes random guesses (equal probability for all classes). We can calculate the initial loss with `-log(1/10)`, returning approximately 2.3.\n",
        "We use `-` since it is negative loss.\n",
        "\n",
        "In summary, the negative loss helps the neural network understand how well it's doing."
      ],
      "metadata": {
        "id": "17X7Qf9c60AK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "loss_fn(y_train[:1], predictions).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OdlMrjS60VY",
        "outputId": "6c362929-d345-4395-e237-760835e9bc87"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.557065"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training, we need to compile the model and optimize it. In this case we are using the \"adam\" model."
      ],
      "metadata": {
        "id": "dkugQuQo-AJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "z0XV3GEZ-NAd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When all this is done, we can start training the neural network. Next up, we'll also cover how to evaluate the model."
      ],
      "metadata": {
        "id": "MjsClRzk-XUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkI5MyQJ-oua",
        "outputId": "0089d2d0-3ef8-4b37-ec77-22f8c744ce7a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0405 - accuracy: 0.9868\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0376 - accuracy: 0.9868\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0356 - accuracy: 0.9880\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0328 - accuracy: 0.9892\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0323 - accuracy: 0.9891\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b382edd7610>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can evaluate it and check its performances"
      ],
      "metadata": {
        "id": "UTvAi_lp-zbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test,  y_test, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AVx_lQY-7yK",
        "outputId": "7767a41f-c6cc-4d83-aaee-30f2deb8ff78"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 1s - loss: 0.0743 - accuracy: 0.9806 - 547ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07434196770191193, 0.9805999994277954]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training it a few times, we can get an accuracy close to 97%. If you repeat this even more, you can approach 98%. I hit this treshold after only training it 15 times."
      ],
      "metadata": {
        "id": "tRp3P5p1_rv6"
      }
    }
  ]
}
